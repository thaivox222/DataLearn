### DE-101 Module 6

## Лабораторная работа по Redshift

Если вы застряли на этапе генерации данных в лабораторной работе по Redshift, то [здесь](Generating_Datasets.md)
обновленный мануал по выполнению задания

Вот такой объём данных в итоге  был загружен в s3  
![lets_pic](/docs/images/S3-gen-data.jpg)

После загрузки сгенерированных данных в мой Redshidt dc2.large 2GB RAM/100GB SSD скорость выполнения запросов следующая:

Таблицы без сжатия, стиля распределения или ключа сортировки: 34,46 сек.  
Таблицы со сжатием, без стиля распределения или ключей сортировки: 27,78 сек.  
Таблицы со сжатием, стилем распределения и ключами сортировки: 19,42 сек.

## Лабораторная работа по Azure

Пришлось пропустить т.к. не могу подтвердить облачный аккаунт (не принимает карты РФ)

## Лабораторная работа по Snowflake

Со времени написания мануала интерфес Снежинки немного изменился. Поэтому указать, например, параметры для file format через модальное окно, как показано на скрине, не получится. Теперь интерфейс предлагает прописать все параметры через конфиг файл.
Вот готовый пример, как прописать в конфиге, чтобы начать загрузку:  

```sql
create file format CSV
    type = CSV
    FIELD_DELIMITER = ','
    ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE
    FIELD_OPTIONALLY_ENCLOSED_BY = '"'
    -- comment = '<comment>'
```  
У меня же и после прописывания этих параметров продолжили выскакивать эксепшены. Чтобы убрать их я прописал уже в самой команде COPY параметр 'ON_ERROR=CONTINUE'   

```sql
copy into trips from @citibike_trips
ON_ERROR=CONTINUE
file_format=CSV;
```  
Результаты загрузки тестовых данных на Warehouse size = Small

Скриншот 1  
![lets_pic](/docs/images/snw_small_load1.jpg)  
Скриншот 2  
![lets_pic](/docs/images/snw_small_load2.jpg)  
Скриншот 3  
![lets_pic](/docs/images/snw_small_load3.jpg)   

А так отработала загрузка тестовых данных после перехода на  Warehouse size = Large  
Скриншот 4  
![lets_pic](/docs/images/snw_large_load1.jpg)  
Скриншот 5  
![lets_pic](/docs/images/snw_large_load2.jpg) 

А это результат отработки одного и того же селекта до и после кэширования  
![lets_pic](/docs/images/snw_cashing_speed.jpg) 

Если в этом запросе выдает ошибку, попробуйте увеличить параметр result_limit c 5 до 15-20  
```sql
set query_id =
(
    select 
        query_id 
    from table(information_schema.query_history_by_session (result_limit=>5))
    where query_text like 'update%' 
    order by start_time 
    limit 1
);
```  
Я записался на этот [курс](https://www.snowflake.com/data-cloud-academy-data-analysts/) от Snowflake Data Academy.  

Курс состоит из серии видео уроков по темам:  

##### SNOWFLAKE 101 (вводные уроки по основам Снежинки)  

![lets_pic](/docs/images/snowflake_cloud_academy-1.jpg)  

##### ANALYSIS AND VISUALIZATION BEST PRACTICES  

![lets_pic](/docs/images/snowflake_cloud_academy-2.jpg)  

##### DATA MANAGEMENT FOR ANALYSTS  

![lets_pic](/docs/images/snowflake_cloud_academy-3.jpg)  

##### ADVANCED ANALYTICS AND EMERGING TRENDS  

![lets_pic](/docs/images/snowflake_cloud_academy-4.jpg)  


По факту в этих видео есть парочка воркшопов, а остальные - много  болтавни с примерами юзкейсов. Практических знаний ноль, зато прокачаешься в том как и с чем эту Снежинку готовят.  


## Решение кейса [Zero to Snowflake](https://github.com/DecisiveData/ZeroToSnowflake)  

В оригинальном кейсе пайплайн выглядел так: 
```python
Salesforce data -> Fivetran replication -> Snowflake warehouse -> Tableau dashboards  
```
Табло я поменял на опенсорсное решение Apache Superset. Во-первых, Табло я практиковался года 2 назад и уже не так интересно, да и как таковой практики по части BI в кейсе нет. Просто скачиваешь готовый .twbx файл и открывешь его в Tableau Desktop.  
А Суперсет я начал изучать недавно и как раз появилась хорошая возможность попрактиковаться в дашбординге на данных из кейса.  

Salesforce - регистрируем аккаунт девелопера и получаем доступ к free account, no credit card needed  
После регистрации наполняем нашу CRM фейковой датой, которую потом и будем гнать по пайплайну  
[https://appexchange.salesforce.com/listingDetail?listingId=a0N3A00000EO5smUAD](https://appexchange.salesforce.com/listingDetail?listingId=a0N3A00000EO5smUAD)  

Fivetran поключаем как партнерский коннектор из Снежинки. Он будет на триальном доступе 14 дней.  

Снежинка у меня была с неиспользованными триальными кредитами с прошлого воркшопа.  

Apache Superset у меня был развернут в облаке VK Cloud.  

Выполнение первых трех шагов пайплайна не вызывает сложностей, делаем все как прописано в кейсе.
А вот подключение Суперсета к Снежинке пошло не так гладко. Предлагаю ссылку на мой блог [Lets Analyse it!](https://lets-analyse-it.blogspot.com/2022/08/apache-superset-snowflake.html), где я набросал подробный гайд, который позволит вам сэкономить пару вечеров за этим делом.  

В итоге получился вот такой дашбордик на основе данных из Salesforce:  

![lets_pic](/docs/images/sales_dashboard_superset.jpg)   

В результате кейса прокачал сразу несколько навыков:  

- пощупал Salesforce в первом приближении  
- потестил Fivetran и попробовал настроить синхронизацию с Snowflake  
- подключил к Snowflake Apache Superset, попрактиковался в создании чартов и сделал демо-дашборд  

## Лабораторная работа по ELT (строим end-to-end решение в облаке)

Поскольку у меня оставалось несколько дней от триального доступа к Fivetran и еще были не выбраны все кредиты на Снежинке, решил что буду использовать эти инструменты. План пайплайна был такой:  
```python
S3 -> Fivetran -> Snowflake warehouse -> dbt  -> Apache Superset
```
Для кейса в Снежинке я создал новые сущности: Database, Schema, User, Password, Role, Warehouse и пошел все это дело прописывать в Fivetran в коннектор Снежинки. Их коннектор устроен таким образом, что сначала выбираешь destination, потом тестируется подключение к нему, а потом уже подключаешь источники. Но тут что-то пошло не так.
Fivetran успешно тестировал пинг и креды моей Снежинки, но при тесте на deafault warehouse выдавал ошибку Websocket connection failed. Гугление похожие кейсы не обнаружило. Отключение/включение VPN и антивируса тоже не дало эффекта.
Вобщем Fivetran повел себя довольно странно. Оставшийся от воркшопа ZeroToSnowflake коннектор работал, но попытка сделать новый или добавить новый соурс в существующий фейлились на этапе теста соединения.
Поэтому решил взять другой инструмент и мой выбор пал на Airbyte. Ранее я его уже тестировал, но он мне показался сыроватым. Видать пришла пора разобраться с ним по-серьезному.
Новый план пайплайна выглядел так:
```python
S3 -> Airbyte -> Snowflake warehouse -> dbt  -> Apache Superset
```
Как только я вбил креды Снежинки и нажал test connction, через пару секунд airbyte выдал, что все норм и можно начинать синкать данные. Это укрепило меня в подозрениях, что у Fivetran специально гадил и не давал закончить тест.

Прикол от Fivetran
![lets_pic](/docs/images/fivetran_wtf.jpg) 

#### Airbyte
Особенностью Airbyte является то, как он складывает данные в таблицу-приёмник. Если у вас есть, к примеру, исходная таблица в 12 столбцов и 100 строк, то в при ее импорте в базу-данных вы получите таблицу из 3 столбцов и 100 строк. 
Два столбца - это служебные, которые добавляет сам Airbyte. В третий записывается в JSON-формате содержимое полей каждой строки.  
![lets_pic](/docs/images/airbyte_row_format.jpg)  

Необходимость дополнительно парсить JSON может испортить впечатления от работы с этим инструментом. К счастью, современные движки баз данных имеют встроенную поддержку json и пару строчек кода в селекте даст нам результат в виде привычной плоской таблицы. Такая поддржка точно есть у Postgres и у Snowflake, возможно у ClickHouse. 
Есть второй вариант - это применения опции  Basic normalisation, но для этого у вас должна быть настроена интеграция Airbyte с dbt и написана sql-модель для трансформации сырых данных. Если вы сможете это сделать, значит вам уже не надо проходить никаких курсов по data engineering.
У Airbye есть несколько детских болезней - при изменении схемы источника (в моём случаем это был S3),
каждый раз приходилось заново вставлять Access Key/Secret Key. Это прям очень бесило. Иногда требовалось удалить источник и создать его с нуля, чтобы connection test проходил. Неудобно и то, что в одном source можно прописать только одну таблицу. И нет функций клонирования. Если у вас 100 таблиц для синхронизации - придётся все 100 внести вручную с самого начала. Боль.
Для решения этой лабы я взял датасет из трех таблиц jaffle shop, которые используются в документации к dbt:

-- jaffle_shop_customers.csv
-- jaffle_shop_orders.csv
-- jaffle_shop_stripe_payment

#### dbt

После того как injestion отработал, можно приступать к трансформации данных. Для этого, как и задумано в пайплайне, я использовал dbt. С ним я уже давно имел знакомство и он был установлен на той же машинке, что и airbyte и superset.
Для начала добавляем новый драйвер для snowflake в то окружение, куда у вас установлен dbt.

```python
pip install dbt-snowflake
```

Потом добавляем новую запись в profiles.yml и финально создаем папку с проектом и собственно dbt_project.yml с параметрами нашего dbt-проекта.
Дополнительно я создал описание источника src_snowflake.yml и приступил к написанию моделей. К таблицам я применил материализацию table, а итоговую витрину данных сделал как view.

#### Superset

Поскольку подключение к Snowflake к Суперсет я отработал еще на прошлом воркшопе ZeroToSnowflake, то это проблем не составило. В качестве датасета подключил витрину и набросал несколько базовых чартов. А потом собрал их на одном дашборде. Не супер-красиво, но для учебных целей сгодится )

![lets_pic](/docs/images/jaffle_shop_dashboard.jpg)

Таким образом, было реелизовано end-to-end решение, начиная от поставки сырых данных, до визуализации результата.
Схема решения (draw.io файл прилагаю)  

![lets_pic](/docs/images/lab_6.6_diagram.jpg)

Какие я получил навыки:

-- намучился с Fivetran (опыт негативный)
-- обновил скилл по работе с Airbyte (прошлый раз он мне не понравился)
-- в очередной раз убедился, что snowflake + dbt = это очень удобно и просто






